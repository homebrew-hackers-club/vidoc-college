---
title: Data Leakage & Model Retention
---

Welcome to Module 2! In Module 1, we established that AI coding assistants offer real productivity gains but introduce serious security risks. We also learned that banning them drives usage underground, making the problem worse.

Now we're diving deep into the specific security risks, starting with one of the most concerning: **data leakage and model retention**.

Here's the uncomfortable reality: every time a developer uses an AI coding assistant, there's potential for sensitive information to leave your organization. Source code, API keys, customer data, proprietary algorithms ‚Äî all of it can be transmitted to third-party servers, sometimes without the developer even realizing it.

Let's understand how this happens and, more importantly, how to prevent it.

## The Scale of the Problem

Before we dive into the details, let's look at some concerning statistics:

- **1 in 5 UK companies** experienced data leakage because of employees using generative AI <sup>[1](#1)</sup>
- In a survey of 8,000 developers, more than 30% of developers said that they don't have the right policies in place to reduce security risks <sup>[2](#2)</sup>
- The **Samsung incident** in 2023 saw employees leak proprietary source code and meeting notes to ChatGPT, leading to a temporary company-wide ban <sup>[3](#3)</sup>

This isn't theoretical ‚Äî organizations are experiencing real data leakage incidents right now.

## Why Data Leakage Matters: Three Critical Dimensions

**1. Source Code is Crown-Jewel Intellectual Property**

Your source code represents years of engineering effort, competitive advantage, and business logic that differentiates your product. When developers paste code into AI assistants, they're potentially sharing:

- **Proprietary algorithms** that took months to develop and optimize
- **Business logic** that encodes your unique approach to solving problems
- **Architectural decisions** that competitors would love to understand
- **Security implementations** that attackers could exploit if exposed
- **API designs and integrations** that reveal your technical ecosystem

Unlike a customer database or employee records, source code is often the *most valuable* asset a technology company possesses. Yet it's routinely shared with AI providers without the same level of protection applied to other sensitive data.

**2. Compliance and Regulatory Exposure**

Data leakage isn't just an intellectual property concern ‚Äî it's a compliance nightmare. Depending on your industry and jurisdiction, unauthorized data transfers can violate:

- **GDPR** (General Data Protection Regulation) ‚Äî Transfers of EU personal data outside approved channels can result in fines up to ‚Ç¨20 million or 4% of global annual revenue, whichever is higher <sup>[4](#4)</sup>
- **CCPA** (California Consumer Privacy Act) ‚Äî Similar restrictions on California resident data with significant penalties
- **HIPAA** (Health Insurance Portability and Accountability Act) ‚Äî Healthcare data shared with unauthorized systems creates breach notification requirements and potential fines
- **SOC 2** ‚Äî Service Organization Control requirements mandate strict data handling and vendor management
- **PCI-DSS** ‚Äî Payment card industry standards prohibit storing or transmitting cardholder data through unapproved systems
- **Industry-specific regulations** ‚Äî Financial services (SOX), defense contractors (ITAR, CMMC), and others have strict data handling requirements

When a developer pastes code containing customer data into ChatGPT, they may have just triggered a reportable compliance violation.

**3. Model Training and Retention Risks**

Even when providers claim they won't use your data for training, the reality is more nuanced:

- **Retention for operational purposes** ‚Äî Many providers retain data temporarily for abuse detection, fraud prevention, and service improvement
- **Metadata and analytics** ‚Äî Even if prompt content isn't stored, metadata about your usage patterns, feature requests, and problem domains may be
- **Third-party subprocessors** ‚Äî Your data may pass through multiple systems and vendors before reaching the model
- **Legal and government requests** ‚Äî Stored data can be subpoenaed or accessed under various legal frameworks
- **Breach exposure** ‚Äî Data stored on provider servers becomes a target for attackers <sup>[5](#5)</sup>

<Mermaid chart={`
flowchart TB
    subgraph dev[" "]
        A[Developer IDE<br/>Code, API Keys, Data]
    end
    
    subgraph provider[" "]
        B[AI Provider Servers]
    end
    
    A -->|"Prompt + Context"| B
    
    B --> C{Data Processing<br/>Decision Point}
    
    C -->|Enterprise/API Tier<br/>With DPA/Zero-Retention| D["Zero Retention<br/>(or minimal 30-day)"]
    C -->|Free/Personal Tier<br/>No Agreement| E["Training Data<br/>(Used to improve models)"]
    C -->|All Tiers| F["Logging & Monitoring<br/>(Abuse detection)"]
    
    E --> G["Persistent Storage"]
    F --> H["Temporary Storage"]
    D --> I["Deleted After Processing"]
    
    G -.->|Potential Exposure| J["Model Outputs<br/>(To other users)"]
    G -.->|Legal Request| K["Subpoena Access"]
    B -.->|Security Incident| L["Data Breach"]
    B -.->|Internal| M["Employee Access"]
    
    style A fill:#3b82f6,stroke:#1e40af,color:#fff
    style B fill:#f59e0b,stroke:#d97706,color:#fff
    style D fill:#10b981,stroke:#059669,color:#fff
    style E fill:#dc2626,stroke:#b91c1c,color:#fff
    style F fill:#6b7280,stroke:#4b5563,color:#fff
    style G fill:#dc2626,stroke:#b91c1c,color:#fff
    style H fill:#6b7280,stroke:#4b5563,color:#fff
    style I fill:#10b981,stroke:#059669,color:#fff
    style J fill:#dc2626,stroke:#b91c1c,color:#fff
    style K fill:#f59e0b,stroke:#d97706,color:#fff
    style L fill:#dc2626,stroke:#b91c1c,color:#fff
    style M fill:#f59e0b,stroke:#d97706,color:#fff
    
    style dev fill:#f8fafc,stroke:#cbd5e1,stroke-width:2px
    style provider fill:#fef3c7,stroke:#fbbf24,stroke-width:2px
    
    linkStyle 0 stroke:#f59e0b,stroke-width:3px
    linkStyle 1,2,3,4 stroke:#6b7280,stroke-width:2px
`} />

## How Data Leakage Happens: Five Common Attack Vectors

Let's explore the most common ways sensitive information leaves your organization through AI coding assistants.

### 1. Direct Prompting with Sensitive Code or Data

This is the most obvious vector but also the most common. Developers actively paste sensitive information into AI chat interfaces or IDE extensions.

**Common scenarios:**

**Debugging with real credentials:**
```python
# Developer asks AI: "Why isn't this working?"
import stripe
stripe.api_key = "sk_live_51KAbc123xyzRealKeyHere"
customer = stripe.Customer.create(email="user@example.com")
```

The developer just shared a live production API key with a third-party AI service.

**Sharing proprietary business logic:**
```javascript
// Developer asks: "How can I optimize this pricing algorithm?"
function calculateDynamicPrice(user, product) {
  const basePrice = product.price;
  const customerLifetimeValue = user.totalSpent;
  const competitorPrice = getCompetitorPrice(product.id);
  
  // Proprietary pricing formula that represents months of A/B testing
  return basePrice * (1 - customerLTV * 0.02) * 
         Math.min(1.1, competitorPrice / basePrice);
}
```

This proprietary pricing algorithm ‚Äî potentially worth millions in competitive advantage ‚Äî is now in a third-party system.

**Exposing database schemas:**
```sql
-- Developer asks: "How do I optimize this query?"
SELECT 
  users.id, users.email, users.payment_method_token,
  subscriptions.plan_tier, subscriptions.mrr,
  usage_events.feature_flags
FROM users
JOIN subscriptions ON users.id = subscriptions.user_id
JOIN usage_events ON users.id = usage_events.user_id
WHERE users.enterprise_contract_id = 'ACME_CORP_2024';
```

This query reveals your database schema, business model (MRR-based subscriptions), enterprise customers, and internal identifiers.

**Real-world incident:** In 2023, Samsung banned ChatGPT after engineers leaked semiconductor design code and internal meeting recordings while asking the AI to help optimize code and transcribe meetings <sup>[3](#3)</sup>. The exposure potentially revealed proprietary chip designs to a third party.

### 2. IDE Extensions Auto-Sending Context

Modern AI coding assistants integrate deeply with your IDE to provide better suggestions. But this means they need access to your code ‚Äî and different tools handle this access differently.

**What gets sent automatically:**

- **Current file content** ‚Äî The file you're actively editing
- **Open files** ‚Äî Other files you have open in tabs
- **Project structure** ‚Äî File names, directory structure, imports
- **Git history** ‚Äî Recent commits and changes (in some tools)
- **Workspace metadata** ‚Äî Project type, frameworks, dependencies
- **Surrounding code** ‚Äî Functions and classes around your cursor position

Some tools send this context to their servers to generate better suggestions. Others process it locally. Many developers don't realize how much context is being transmitted.

**Configuration matters immensely:**

The difference between personal/free tiers and enterprise/business tiers is significant:

- **Personal/Free tiers** ‚Äî Typically retain data for model improvement unless you explicitly opt out; may have 30+ day retention periods
- **Enterprise/Business tiers** ‚Äî Usually offer zero-retention options, no-training commitments, and configurable data controls

**Baseline settings to verify:**
- Does the tool retain prompts/suggestions by default?
- Is there an explicit no-training commitment in writing?
- Can you configure privacy modes or zero-retention?
- What are the data retention periods?

The problem? Most developers use personal accounts with default settings, not enterprise tiers with data protection guarantees.

### 3. Telemetry, Crash Reports, and Usage Analytics

Beyond the intentional code sharing, many AI tools collect telemetry data to improve their products:

- **Usage patterns** ‚Äî What features you use, how often, what types of code
- **Error reports** ‚Äî When the tool crashes or encounters issues, reports may include code snippets
- **Performance metrics** ‚Äî Response times, which can reveal system architecture
- **A/B testing data** ‚Äî Different suggestion algorithms tested on your code
- **Feature adoption metrics** ‚Äî What parts of your codebase trigger which features

While this data is typically aggregated and anonymized, there have been cases where telemetry systems accidentally capture more information than intended.

### 4. Third-Party Connectors and AI Agents

The newest generation of AI tools goes beyond code completion ‚Äî they integrate with your entire development ecosystem:

- **Jira/Linear integration** ‚Äî Reading ticket descriptions, acceptance criteria, comments
- **Slack/Teams integration** ‚Äî Accessing conversations, incident postlogs, architecture discussions
- **Google Drive/Confluence** ‚Äî Reading design docs, technical specifications, runbooks
- **GitHub/GitLab** ‚Äî Accessing issues, pull requests, code review comments
- **Calendar integration** ‚Äî Understanding meeting context, attendee lists, project timelines

These integrations create a new attack surface: **prompt injection attacks**. An attacker can plant malicious instructions in a Jira ticket, Google Doc, or Slack message that, when read by an AI agent, cause it to exfiltrate data or perform unauthorized actions <sup>[9](#9)</sup>.

For example, hidden instructions in a Jira ticket could tell an AI agent to search for API keys across your systems and report them back ‚Äî and the AI might actually execute those instructions, treating them as legitimate commands.

This is such a significant threat that we dedicate an entire chapter to it. **Chapter 6: Prompt Injection & Ecosystem Exploits** provides a comprehensive exploration of these attacks, how they work, real-world exploitation examples, and defensive strategies.

### 5. Copy-Paste from Confidential Documents

Developers often copy code from internal wikis, documentation, or incident postmortems to ask AI for help. Even with redaction attempts, sensitive information leaks:

- **Insufficient redaction** ‚Äî Replacing specific names but leaving identifiable patterns
- **Metadata exposure** ‚Äî File names like `acme-corp-api-integration.py` reveal customer relationships
- **Context clues** ‚Äî "Our proprietary algorithm for..." followed by the algorithm
- **Re-identification** ‚Äî Anonymized data that can be de-anonymized through correlation

**Example of insufficient redaction:**

```python
# Developer thinks this is safe:
def process_payment(user_id, amount):
    # Call [REDACTED] payment processor
    response = requests.post(
        "https://api.[REDACTED].com/v1/charges",
        headers={"Authorization": f"Bearer {REDACTED_KEY}"},
        json={"amount": amount, "user": user_id}
    )
```

But the API endpoint structure (`/v1/charges`) and authentication pattern (`Bearer` token) already reveal they're using Stripe. Combined with other context clues, redaction often fails to protect sensitive information.

## Understanding Retention: What Really Happens to Your Data

When you use an AI coding assistant, your data doesn't just disappear after the response is generated. Understanding what happens to it requires untangling three distinct concepts that are often confused: **training**, **retention**, and **logging**.

### Training vs. Retention vs. Logging

**Training** means your data is used to improve the AI model itself. Your code becomes part of the training dataset that makes the model better at generating code. This is the most concerning form of data use because:

- Your proprietary code patterns could be suggested to other users
- Unique identifiers, API designs, or algorithms might be reproduced
- The data becomes permanently embedded in the model's weights
- There's no way to "delete" your data once it's been trained into a model

**Retention** means the AI provider stores your prompts and responses for a defined period. This storage serves various purposes:

- Showing conversation history in the UI
- Debugging issues and improving service quality
- Abuse and fraud detection
- Legal compliance and incident response
- Potential future training (if terms allow)

Retention is temporary ‚Äî data is stored for days, weeks, or months, then deleted according to the provider's policy.

**Logging** refers to operational logs that capture metadata about your usage:

- Timestamps, request IDs, user IDs
- Error codes and performance metrics
- Feature flags and A/B test assignments
- IP addresses and authentication events
- Potentially sanitized snippets for debugging

Logs typically retain less information than full retention, but they can still expose sensitive details about your organization's usage patterns and systems.

### The Tier Difference: Personal vs. Enterprise

The biggest factor in data handling is which tier of service you're using. The difference between personal and enterprise accounts is enormous:

| Feature | üÜì Free/Personal | üíº Team/Business | üè¢ Enterprise |
|---------|:----------------:|:----------------:|:-------------:|
| **Data Retention** | üî¥ Indefinite<br/>(unless opt-out) | üü° 30-90 days | üü¢ Zero or minimal<br/>(30 days max) |
| **Training Usage** | üî¥ Yes<br/>(opt-out required) | üü° No<br/>(by default) | üü¢ Never<br/>(contractual guarantee) |
| **DPA/SCCs** | üî¥ None | üü° Limited<br/>(basic terms) | üü¢ Full DPA<br/>(GDPR compliant) |
| **Compliance** | üî¥ None | üü° SOC 2 | üü¢ SOC 2, ISO 27001<br/>HIPAA BAA available |
| **Context Filtering** | üî¥ None | üü° Basic<br/>(manual config) | üü¢ Advanced<br/>(automated PII detection) |
| **Audit Logs** | üî¥ None | üü° Limited | üü¢ Comprehensive<br/>(exportable) |
| **Support** | üî¥ Community | üü° Email | üü¢ Dedicated + SLA |
| **Cost** | Free-$20/month | $25-40/user/month | $30-60/user/month |

**üö® Critical Difference:** The $10-40/month cost difference may seem small, but the data protection guarantees are fundamentally different. Free tiers treat your data as training material; enterprise tiers provide contractual commitments and compliance features.

**Key Takeaway:** Always use enterprise/business tiers with explicit no-training commitments and DPAs. Personal accounts should be blocked for work-related coding.

### What to Verify Before Approving a Tool

Before you approve any AI coding assistant for organizational use, verify these critical details:

**1. Training commitments**
- Does the provider commit in writing to not training on your data?
- Is this commitment in the Terms of Service or a separate Data Processing Agreement (DPA)?
- Are there exceptions for abuse prevention or service improvement?

**2. Retention policies**
- How long are prompts and responses stored?
- Can retention be reduced to zero or near-zero?
- What happens to data after the retention period?
- Is data truly deleted or just marked for deletion?

**3. Data Processing Agreements (DPAs)**
- Is there a formal DPA that meets GDPR requirements?
- Does it include Standard Contractual Clauses (SCCs) for EU data transfers?
- Who are the subprocessors that might access your data?
- What are their security certifications?

**4. Data residency and sovereignty**
- Where are the servers physically located?
- Can you specify a region (EU, US, specific AWS region)?
- Does data ever leave that region, even for processing?
- Are there government access concerns (Cloud Act, GDPR conflicts)?

**5. Security certifications**
- SOC 2 Type II attestation (and how recent)
- ISO 27001 certification
- Industry-specific compliance (HIPAA BAA, PCI-DSS, FedRAMP)
- Third-party security audits and penetration testing

**6. Logging and telemetry**
- What operational logs are kept and for how long?
- What telemetry is collected?
- Can telemetry be disabled?
- Who has access to logs?

**7. Breach notification**
- How quickly will you be notified of a breach?
- What incident response procedures are in place?
- Is there cyber insurance coverage?

Don't just read the marketing materials ‚Äî actually review the Terms of Service, DPA, and privacy policy. Better yet, have your legal and security teams review them.

## Defense-in-Depth (Condensed)

Use the earlier "Safe‚Äëby‚Äëdefault rules" and "Technical controls that help" as the single source of truth. At a glance:

- **AI gateway / LLM proxy**: allowlist providers/models, redact PII/secrets, log prompts/responses, enforce policies, rate limit <sup>[12](#12) [13](#13) [14](#14) [15](#15)</sup>
- **Context minimization**: limit IDE context; avoid sending `.env`, secrets, credentials; close sensitive files
- **Secrets scanning**: in‚Äëeditor, pre‚Äëcommit, CI; block on detection
- **DLP controls**: inspect egress to AI domains; detect API keys/tokens; alert/block
- **Provenance tagging**: mark AI‚Äëassisted diffs; require enhanced review
- **Detect & respond**: audit AI usage, alert on anomalies, run a clear incident playbook

**Reference architecture:**

<Mermaid chart={`
graph LR
    A[Developer IDE] --> B[Corporate Network]
    B --> C[AI Gateway/Proxy]
    C --> D[Approved AI Provider]
    C --> E[Audit Log]
    E --> F[SIEM]
    C --> G[Redaction Engine]
    C --> H[Policy Enforcement]
    C --> I[Rate Limiter]
    
    style C fill:#f9f,stroke:#333,stroke-width:3px
    style D fill:#bbf,stroke:#333,stroke-width:2px
    style A fill:#bfb,stroke:#333,stroke-width:2px
`}/>

## Practical Redaction: Examples You Can Use Today

Developers often need to share code structure with AI while protecting sensitive information. Here's how to do it effectively:

### Example 1: Authentication Code

**Before (unsafe):**
```python
# Developer asks: "Why is this authentication failing?"
import requests

API_KEY = "sk_live_51KAbc123xyzRealProductionKey"
SECRET = "whsec_789RealWebhookSecret"

def authenticate_user(username, password):
    response = requests.post(
        "https://api.stripe.com/v1/auth",
        headers={"Authorization": f"Bearer {API_KEY}"},
        json={"user": username, "pass": password, "webhook_secret": SECRET}
    )
    return response.json()
```

**After (safe with placeholders):**
```python
# Safe version to share with AI
import requests

API_KEY = "<REDACTED_STRIPE_KEY>"
SECRET = "<REDACTED_WEBHOOK_SECRET>"

def authenticate_user(username, password):
    response = requests.post(
        "https://api.payment-provider.com/v1/auth",
        headers={"Authorization": f"Bearer {API_KEY}"},
        json={"user": username, "pass": password, "webhook_secret": SECRET}
    )
    return response.json()
```

The AI can still help debug the logic without seeing production credentials.

### Example 2: Database Queries

**Before (unsafe - exposes schema and PII):**
```sql
-- Query is slow, need optimization help
SELECT 
  users.id,
  users.email,
  users.social_security_number,
  users.credit_card_last_4,
  subscriptions.plan_id,
  subscriptions.monthly_revenue,
  enterprise_contracts.company_name,
  enterprise_contracts.contract_value
FROM users
JOIN subscriptions ON users.id = subscriptions.user_id
JOIN enterprise_contracts ON users.company_id = enterprise_contracts.id
WHERE enterprise_contracts.company_name = 'Acme Corporation'
  AND subscriptions.monthly_revenue > 10000
  AND users.created_at > '2024-01-01';
```

**After (safe - anonymized structure):**
```sql
-- Anonymized version that still shows the query structure
SELECT 
  u.id,
  u.email_hash,
  u.identifier_field,
  u.payment_field,
  s.plan_type,
  s.revenue_amount,
  e.company_identifier,
  e.contract_amount
FROM users u
JOIN subscriptions s ON u.id = s.user_id
JOIN enterprise_contracts e ON u.company_id = e.id
WHERE e.company_identifier = '<CUSTOMER_ID>'
  AND s.revenue_amount > <THRESHOLD>
  AND u.created_at > '<DATE>';
```

### Example 3: API Integration Code

**Before (unsafe - reveals partner integration):**
```javascript
// Integration with secret partner
const PARTNER_API = "https://api.megacorp-private.com/v2/internal";
const PARTNER_KEY = "pk_live_abc123xyz789";
const PARTNER_WEBHOOK = "https://our-domain.com/webhooks/megacorp-secret";

async function syncCustomerData(customerId) {
  const customer = await db.customers.findOne({
    id: customerId,
    megacorp_customer_id: { $exists: true }
  });
  
  return axios.post(`${PARTNER_API}/customers/sync`, {
    auth: PARTNER_KEY,
    customer: {
      id: customer.megacorp_customer_id,
      revenue: customer.lifetime_value,
      tier: customer.enterprise_tier
    },
    callback: PARTNER_WEBHOOK
  });
}
```

**After (safe - generic pattern):**
```javascript
// Generic version showing structure only
const PARTNER_API = "<PARTNER_API_URL>";
const PARTNER_KEY = "<PARTNER_API_KEY>";
const PARTNER_WEBHOOK = "<OUR_WEBHOOK_URL>";

async function syncCustomerData(customerId) {
  const customer = await db.customers.findOne({
    id: customerId,
    partner_customer_id: { $exists: true }
  });
  
  return axios.post(`${PARTNER_API}/customers/sync`, {
    auth: PARTNER_KEY,
    customer: {
      id: customer.partner_customer_id,
      revenue: customer.lifetime_value,
      tier: customer.tier_level
    },
    callback: PARTNER_WEBHOOK
  });
}
```

### Example 4: Configuration Files

**Before (unsafe):**
```yaml
# production.yml
database:
  host: prod-db-master-01.us-east-1.rds.amazonaws.com
  username: admin_prod_user
  password: "SuperSecret123!Production"
  database: customer_data_prod

stripe:
  secret_key: sk_live_51KAbc123xyz
  webhook_secret: whsec_789def

aws:
  access_key: AKIAIOSFODNN7EXAMPLE
  secret_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

**After (safe):**
```yaml
# Example configuration structure
database:
  host: <DB_HOST>
  username: <DB_USERNAME>
  password: <DB_PASSWORD>
  database: <DB_NAME>

payment_provider:
  secret_key: <PAYMENT_SECRET>
  webhook_secret: <WEBHOOK_SECRET>

cloud_provider:
  access_key: <ACCESS_KEY>
  secret_key: <SECRET_KEY>
```

## Data Classification: What Can Be Shared?

Here's a practical guide for what's safe to share with AI assistants:

| Data Type | Safe to Share? | Requirements & Notes |
|-----------|:--------------:|----------------------|
| **Public documentation** | ‚úÖ Yes | Still avoid internal system names, customer references |
| **Open source code** | ‚úÖ Yes | Verify license compatibility before using AI suggestions |
| **Non-sensitive boilerplate** | ‚úÖ Yes | Standard CRUD, configs, tests for public features |
| **Synthetic/test data** | ‚úÖ Yes | Ensure it's truly synthetic, not production data with names changed |
| **Internal business logic** | üü° Caution | Only with approved enterprise tier; no proprietary algorithms |
| **Database schemas** | üü° Caution | Anonymize table/column names; never include data samples |
| **API designs** | üü° Caution | Generic patterns OK; specific endpoint URLs/auth reveal too much |
| **Customer data (any PII)** | ‚ùå Never | Including names, emails, phone, addresses, IDs |
| **PHI (healthcare data)** | ‚ùå Never | HIPAA violation; use synthetic data only |
| **Financial information** | ‚ùå Never | Payment details, account numbers, transaction data |
| **Credentials & secrets** | ‚ùå Never | API keys, passwords, tokens, certificates, connection strings |
| **Proprietary algorithms** | ‚ùå Never | Core IP that differentiates your product |
| **Security implementations** | ‚ùå Never | Auth logic, encryption keys, security configs |
| **Customer lists/partners** | ‚ùå Never | Reveals business relationships and customer base |

## The Developer's Pre-Flight Checklist

Before asking AI for help, run through this checklist:

**‚úÖ Task Eligibility**
- [ ] This task is approved for AI use per Module 1 framework
- [ ] This is not security-critical code
- [ ] This is not proprietary business logic

**‚úÖ Tool Configuration**
- [ ] I'm using an approved AI tool (from company list)
- [ ] I'm signed in with my work account (not personal)
- [ ] Enterprise tier with no-training commitment is active
- [ ] IDE context filters are configured correctly

**‚úÖ Data Protection**
- [ ] No API keys, tokens, or credentials in my prompt
- [ ] No customer PII, PHI, or financial data
- [ ] Sensitive values replaced with `<REDACTED>` or `<PLACEHOLDER>`
- [ ] File paths don't reveal confidential project names
- [ ] No proprietary algorithms or core business logic

**‚úÖ Context Minimization**
- [ ] IDE is only sending the minimal necessary context
- [ ] Excluded `.env`, `secrets/`, `config/credentials/` files
- [ ] Not sharing entire codebase, just relevant function/file
- [ ] Removed unnecessary comments that might contain sensitive info

**‚úÖ Review & Compliance**
- [ ] I will mark this code as AI-assisted in my PR
- [ ] I will review all AI suggestions before committing
- [ ] I will run security scans (SAST/secrets scanning)
- [ ] I understand I'm accountable for any code I commit

**If you can't check all boxes, stop and either:**
- Redact more information
- Use a different approach that doesn't require AI
- Consult with your security team

## What To Do If You Accidentally Leaked Data

Despite best efforts, accidents happen. Here's your incident response playbook:

**Immediate (within 1 hour):**

1. **Stop using the tool** ‚Äî Don't make things worse by continuing
2. **Document what was shared** ‚Äî Save screenshots, copy the prompt, note the timestamp
3. **Notify your security team** ‚Äî Follow your incident response process
4. **Identify exposed assets** ‚Äî What secrets, PII, or proprietary code was included?

**Short-term (within 24 hours):**

5. **Rotate exposed credentials** ‚Äî All API keys, tokens, passwords that were shared
6. **Delete conversation history** ‚Äî If the tool allows, delete the session
7. **Contact the AI provider** ‚Äî Request data deletion (enterprise tiers usually comply faster)
8. **Assess compliance impact** ‚Äî Is this a reportable breach under GDPR/HIPAA/etc.?

**Medium-term (within 1 week):**

9. **Review access logs** ‚Äî Check if exposed credentials were used maliciously
10. **Update affected systems** ‚Äî If proprietary logic was exposed, consider code changes
11. **Enhanced monitoring** ‚Äî Watch for unusual activity on affected systems
12. **Post-incident training** ‚Äî Learn from the mistake and update team practices

**Long-term (ongoing):**

13. **Update policies** ‚Äî Add specific guidance to prevent similar incidents
14. **Implement technical controls** ‚Äî Add guardrails that would have caught this
15. **Regular audits** ‚Äî Review AI usage logs for potential leakage
16. **Continuous training** ‚Äî Keep the team aware of data protection practices

## Key Takeaways

Before moving to the next chapter, make sure you understand:

- **Data leakage is real and happening** ‚Äî 38% of employees share sensitive work data with AI tools without permission
- **Five main attack vectors** ‚Äî Direct prompting, IDE auto-send, telemetry, third-party connectors, copy-paste
- **Training ‚â† Retention ‚â† Logging** ‚Äî Understanding these distinctions is critical for risk assessment
- **Personal vs. Enterprise tiers** ‚Äî The data protection difference is enormous; always use enterprise
- **Defense in depth** ‚Äî No single control is sufficient; layer policy, technical controls, and detection
- **AI gateway/proxy is critical** ‚Äî Centralized control, redaction, logging, and policy enforcement (Cloudflare, Kong, Treeline)
- **Context filtering matters** ‚Äî Configure IDE extensions to minimize what they send
- **Secrets scanning everywhere** ‚Äî In-editor, pre-commit, and CI/CD
- **Proper redaction is an art** ‚Äî Replace sensitive values while preserving structure
- **Have an incident response plan** ‚Äî Know what to do when data leaks

<Woz 
title="Recap" 
description="Test your understanding üîê" 
context={`Ask user this question:
Imagine a developer on your team just pasted production database credentials into ChatGPT while debugging. Walk me through your incident response: what are the first 3 actions you take, and what technical controls would have prevented this?`}
prompt="Ask me a question to test my understanding of the material."
/>

---

## Sources and Further Reading

<a id="1">[1]</a> **BM Business Matters (2024)** ‚Äì [1 in 5 organisations have had company data exposed by an employee using AI tools such as ChatGPT](https://bmmagazine.co.uk/in-business/1-in-5-organisations-have-had-company-data-exposed-by-an-employee-using-ai-tools-such-as-chatgpt/)

<a id="2">[2]</a> **Stack Overflow (2024)** ‚Äì [Developer Survey 2024: AI Sentiment and Usage](https://survey.stackoverflow.co/2024/ai#sentiment-and-usage-ai-select)

<a id="3">[3]</a> **Bloomberg (2023)** ‚Äì [Samsung Bans ChatGPT After Staff Leaks Chip Design Data](https://www.bloomberg.com/news/articles/2023-05-02/samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak)

<a id="4">[4]</a> **GDPR.eu** ‚Äì [What are the GDPR Fines?](https://gdpr.eu/fines/)

<a id="5">[5]</a> **Forbes (2025)** ‚Äì [DeepSeek Data Leak Exposes 1 Million Sensitive Records](https://www.forbes.com/sites/larsdaniel/2025/02/01/deepseek-data-leak-exposes--1000000-sensitive-records/)

<a id="6">[6]</a> **GitHub** ‚Äì [Copilot for Business: Privacy and Data Handling](https://github.com/features/copilot/copilot-business)

<a id="7">[7]</a> **Cursor** ‚Äì [Privacy Mode Documentation](https://cursor.sh/privacy)

<a id="8">[8]</a> **OpenAI** ‚Äì [ChatGPT Enterprise Privacy and Data Control](https://openai.com/enterprise-privacy/)

<a id="9">[9]</a> **SecurityWeek (2024)** ‚Äì [Major Enterprise AI Assistants Can Be Abused for Data Theft, Manipulation](https://www.securityweek.com/major-enterprise-ai-assistants-abused-for-data-theft-manipulation/)

<a id="10">[10]</a> **Microsoft Azure** ‚Äì [Azure OpenAI Service Data Privacy](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy)

<a id="11">[11]</a> **AWS** ‚Äì [Amazon Bedrock Security and Privacy](https://aws.amazon.com/bedrock/security-compliance/)

<a id="12">[12]</a> **Cloudflare** ‚Äì [AI Gateway: Control Plane for AI Applications](https://workers.cloudflare.com/product/ai-gateway)

<a id="13">[13]</a> **Kong** ‚Äì [Announcing Kong AI Gateway](https://konghq.com/blog/product-releases/announcing-kong-ai-gateway)

<a id="14">[14]</a> **Google Cloud** ‚Äì [Introducing Secure Web Proxy for Egress Traffic Protection](https://cloud.google.com/blog/products/identity-security/introducing-secure-web-proxy-for-egress-traffic-protection)

<a id="15">[15]</a> **Treeline** ‚Äì [Treeline Proxy: Prevent PII and Secrets Leakage](https://treelineproxy.io/)

### Additional Resources

- **OWASP Top 10 for LLM Applications** ‚Äì [https://owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- **NIST AI Risk Management Framework** ‚Äì Guidance on managing AI-related risks
- **LiteLLM Proxy Documentation** ‚Äì Open-source LLM proxy implementation
- **Portkey AI Gateway** ‚Äì Open-source AI gateway documentation
- **Your AI provider's documentation** ‚Äì Always read the specific terms for your tier
- **Synthetic data generation tools** ‚Äì For creating safe test datasets


