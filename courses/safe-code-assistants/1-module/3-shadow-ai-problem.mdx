---
title: The Shadow AI Problem
---

So far, we've covered why developers love AI coding assistants (Chapter 1) and the serious security risks they introduce (Chapter 2). If you're a security leader or engineering manager, your first instinct might be: "We should just ban these tools."

It's a reasonable reaction. Studies show AI assistance can increase the likelihood that insecure code appears in submissions and reviews. If a meaningful share of AI-generated suggestions contain vulnerabilities—and many developers don't spot them—why not just prohibit their use? <sup>[2](#2) [3](#3)</sup>

Here's the uncomfortable truth: **banning AI tools doesn't work.** In fact, it often makes the problem worse.

## The Shadow AI Phenomenon

When organizations ban AI coding assistants, developers don't stop using them — they just stop telling you about it.

This is called **Shadow AI**: the unauthorized use of AI systems outside official channels, without security oversight, and often in violation of company policy. <sup>[11](#11)</sup> Unlike traditional shadow IT, which involves any unsanctioned software or hardware, shadow AI specifically refers to AI tools, platforms, and applications—introducing unique concerns around data management, model outputs, and decision-making. <sup>[11](#11)</sup>

### Why Developers Ignore Bans

Let's be honest about the incentives:

- **Productivity pressure is real.** Developers are measured on velocity, features shipped, and bugs closed. If a tool makes them 2x faster on certain tasks, they'll use it — policy or no policy. <sup>[12](#12)</sup>
- **The tools are everywhere.** ChatGPT is a free website. GitHub Copilot has a personal tier. Cursor is a download away. Unlike enterprise software that requires procurement and IT setup, AI assistants are consumer-grade tools that anyone can access in minutes. <sup>[13](#13)</sup>
- **Enforcement is challenging.** How do you detect if a developer is using ChatGPT in another browser tab? Or running Copilot on their personal laptop? Or asking Claude to review code during their lunch break? <sup>[11](#11)</sup>
- **Peer effects matter.** When developers see colleagues shipping faster with AI assistance, they feel pressure to use the same tools or fall behind. <sup>[12](#12)</sup>
- **Industry normalization.** With up to 96% of enterprise employees now using generative AI applications and 75% of global knowledge workers using generative AI, blanket bans feel out of touch with current practice. <sup>[1](#1) [11](#11) [12](#12)</sup>

### The Statistics on Shadow AI

Research shows the scope of the problem is alarming:

- **44% of employees have used AI in ways that contravene policies and guidelines**, indicating significant shadow AI prevalence in organizations. <sup>[12](#12)</sup>
- **38% of employees acknowledge sharing sensitive work information with AI tools without their employers' permission.** <sup>[8](#8)</sup>
- **Almost half of employees admit to uploading sensitive company information to unauthorized platforms.** <sup>[12](#12)</sup>
- **57% of employees have made mistakes due to AI, and 58% have relied on AI output without evaluating its accuracy.** <sup>[12](#12)</sup>
- **Only 41% of employees report that their organization has a policy guiding the use of GenAI**, highlighting a huge gap in guardrails. <sup>[12](#12)</sup>
- **67% of knowledge workers have personal AI subscriptions they use for work tasks.** <sup>[11](#11)</sup>

The bottom line: If you ban AI tools without providing alternatives, you're not stopping usage — you're just losing visibility and control. <sup>[9](#9)</sup>

## Why Shadow AI Is More Dangerous Than Managed AI

When developers use AI tools in secret, you lose all the safeguards you'd have with managed adoption. <sup>[10](#10)</sup>

**1. No Visibility Into What's Being Shared**

Developers using unauthorized tools may be:

- Copying proprietary code into public chat interfaces for debugging
- Pasting customer data into AI tools for help writing queries <sup>[11](#11)</sup>
- Sharing API keys or credentials when asking for help with authentication <sup>[11](#11)</sup>
- Uploading internal documentation to get AI explanations

**Without visibility, you don't know what sensitive information is leaving your organization.** In fact, one study found that 1 in 5 UK companies experienced data leakage because of employees using generative AI. <sup>[8](#8)</sup>

**2. No Control Over Data Retention and Training**

When employees use unauthorized AI tools, organizations face serious data governance issues:

- **Sensitive information may be stored indefinitely** on third-party servers without proper data processing agreements. <sup>[12](#12)</sup>
- **Data may be used to train AI models**, potentially exposing proprietary information to competitors using the same tools. <sup>[11](#11) [12](#12)</sup>
- **Data exfiltration can occur** when AI providers store or reuse shared information, leading to uncontrolled data exposure. <sup>[12](#12)</sup>
- **Compliance violations** can result from data being processed in jurisdictions without adequate data protection, potentially violating GDPR, CCPA, HIPAA, or other regulations. <sup>[8](#8) [12](#12)</sup>

Enterprise agreements can include strict retention controls and no-training commitments, though some providers may retain minimal data for abuse/fraud detection. Personal accounts typically don't offer these protections. <sup>[4](#4) [5](#5) [6](#6)</sup>

**3. No Code Review Standards**

When AI usage is secret, there's no process for:

- Identifying AI-generated code during review
- Applying extra scrutiny to code from AI assistants
- Running additional security scans on AI-written code
- Training reviewers on what to look for

AI-generated code slips through review processes designed for human-written code. <sup>[10](#10)</sup>

**4. No Security Tooling Integration**

Managed AI adoption can include:

- IDE plugins that scan AI suggestions for vulnerabilities before insertion
- CI/CD gates that flag AI-generated code for extra review
- SAST rules tuned for patterns common in AI-generated code
- Dependency and license scanners for AI-suggested packages
- Central AI proxy with redaction, logging, allowlisting, and provider controls

Shadow AI users get none of this — they're on their own. <sup>[13](#13)</sup>

**5. No Training or Best Practices**

Organizations with official AI programs can train developers on:

- How to prompt AI securely (avoiding sensitive data in prompts)
- When to use vs. avoid AI assistance
- How to review AI-generated code for security issues
- Which tools are approved and why

Shadow AI users figure it out on their own, often making preventable mistakes. <sup>[12](#12) [13](#13)</sup>

## Real-World Examples of Shadow AI Gone Wrong

Let's look at some cautionary tales:

### Example 1: The Credential Leak

A developer at a fintech company, frustrated by slow IT approvals, used ChatGPT to debug an authentication issue. They pasted their code into ChatGPT — including an API key for their production database. <sup>[11](#11)</sup>

That API key was now in the conversation history. Enterprise/API tiers disable training on business data by default and provide retention controls, but the developer used a personal consumer account where prompts may be used to improve models unless they opt out; retention policies also differ by tier. <sup>[4](#4) [5](#5)</sup>

The company only discovered the leak months later during a security audit. By then, the API key had been rotated multiple times, but the original credential had been exposed for weeks.

### Example 2: The Licensing Landmine

A startup was building a proprietary SaaS product. A developer, working late to meet a deadline, used GitHub Copilot (on a personal account, against company policy) to generate code for a critical feature.

Copilot suggested code that closely resembled a GPL-licensed open source project. The developer, not understanding licensing implications, merged it.

Months later, during due diligence for a funding round, the legal team discovered incompatible open-source licensing in their proprietary codebase. They had to rewrite significant portions of the product, delaying their raise by months. While exact reproduction rates are debated, there is a documented risk of inadvertently introducing code that conflicts with your licensing requirements; SCA tooling in CI can mitigate this. <sup>[3](#3)</sup>

### Example 3: The Customer Data Exposure

A healthcare company had strict policies against using unapproved AI tools due to HIPAA compliance requirements. A developer, stuck on a complex SQL query involving patient records, used an AI chatbot with real patient data (names removed, but other identifying information intact). <sup>[11](#11)</sup>

This likely constituted a HIPAA violation and a reportable incident, even without evidence of malicious access, because partially de-identified patient data can still be PHI. Proper de-identification under HIPAA requires removing specific identifiers or expert determination. <sup>[7](#7)</sup>

### Example 4: The DeepSeek Breach

In January 2025, the AI chatbot platform DeepSeek suffered a significant breach where more than 1 million records—including chat logs, API keys, and backend system details—were exposed due to misconfigured infrastructure. <sup>[12](#12)</sup> While the breach was not caused by employee uploads, it highlighted how easily proprietary data can be compromised when AI tools operate outside formal governance. The incident triggered regulatory scrutiny and reputational fallout, underscoring the need for centralized oversight and secure AI environments. <sup>[12](#12)</sup>

**The common thread in all these cases: The organizations had banned AI tools, but developers used them anyway, creating risks that could have been managed with proper policies and oversight.**

## Understanding Why Shadow AI Happens

Before we can solve shadow AI, we need to understand why it happens. <sup>[11](#11)</sup>

### The Speed Mismatch

Consider this timeline: Employees discovered ChatGPT in November 2022 and were using it productively by December. Meanwhile, IT departments formed AI governance committees in January 2023, delivered preliminary findings in June, recommended pilot programs in September, and might approve the first official AI tool sometime in 2025. <sup>[12](#12) [14](#14)</sup>

**The business can't wait years for IT to figure out an AI strategy.** They have quarterly targets, customer demands, and competitive pressures. Every day without AI assistance is a day they fall further behind. <sup>[11](#11)</sup>

### The Innovation Bottleneck

Traditional IT governance was designed for a world where new technologies emerged every few years, not every few weeks. Approval processes that take six months to evaluate a new database are completely inadequate for AI tools that evolve monthly. <sup>[11](#11)</sup>

While IT evaluates whether GPT-4 meets security requirements, employees are already using GPT-4o, Claude 3.5, and three other models IT hasn't even heard of. Governance can't keep up with innovation, so innovation routes around governance. <sup>[11](#11)</sup>

### The Productivity Gap

Here's what IT leaders don't want to admit: **employees using shadow AI are often dramatically more productive than those following official channels.** <sup>[11](#11)</sup> The marketing manager using Claude to write content is producing 5x more than her peers. The developer using Cursor with GPT-4 is shipping features twice as fast. The analyst using ChatGPT for data analysis is finding insights others miss.

When the productivity gain is that dramatic, employees will find a way—using personal devices, personal accounts, and personal judgment. <sup>[11](#11)</sup>

### Organizational and Behavioral Drivers

Shadow AI thrives due to several factors: <sup>[12](#12)</sup>

- **Convenience and simplicity**: Many GenAI tools are free, intuitive, and familiar—mirroring consumer apps.
- **"Bring your consumer life to work" mindset**: Especially among digital natives eager to experiment and stay ahead.
- **Fear of missing out**: Competitive pressures and workplace expectations accelerate adoption.
- **Slow-moving AI strategies**: Ambiguous policies and lack of psychological safety create uncertainty.
- **Perception of harmlessness**: Lack of visible consequences normalizes risky behavior, masking threats like data leakage and compliance failures.

## The Prohibition Doesn't Work Analogy

Think about other times organizations tried to ban popular tools:

- **Personal email** (Gmail, Hotmail) was banned by many companies in the 2000s. Employees used them anyway to send work files.
- **USB drives** were banned for security reasons. Employees used them anyway to transfer files between systems.
- **Cloud storage** (Dropbox, Google Drive) was banned in many enterprises. Employees used it anyway to share large files.

In every case, outright bans failed. What worked was: <sup>[11](#11)</sup>

1. **Understanding why employees wanted the tool** (legitimate productivity needs)
2. **Providing a secure alternative** (enterprise email, secure file transfer, approved cloud storage)
3. **Implementing security controls** (DLP, encryption, access policies)
4. **Training employees** on proper usage

AI coding tools will follow the same pattern.

### When a Ban Is Justified (Temporarily)

There are scenarios where a time-bound, scoped prohibition is appropriate:

- Classified or highly regulated environments pending a vetted solution
- Until an allowlisted provider with DPA/SCCs, logging, and egress controls is live
- For specific high-risk code paths (e.g., cryptography primitives, auth core)
- During active incident response involving model/provider compromise

Pair any temporary ban with a clear 30/60/90-day plan for managed adoption. <sup>[12](#12)</sup>

## The Wrong Approach vs. The Right Approach

**❌ The Wrong Approach**

"We're banning all AI coding tools. Developers caught using them will face disciplinary action."

**What happens:**
- Developers use them anyway
- Usage goes underground
- Security teams have no visibility
- No training or guardrails exist
- Risks increase <sup>[13](#13)</sup>

**✅ The Right Approach**

"We recognize AI coding tools offer productivity benefits. We're implementing a managed AI program with approved tools, security controls, training, and oversight." <sup>[12](#12) [13](#13)</sup>

**What happens:**
- Developers use approved tools
- Usage is visible and monitored
- Security controls are in place
- Training reduces risky behavior
- Risks are managed

## The Path Forward: Managed Adoption

Instead of prohibition, organizations should pursue **managed adoption** with comprehensive governance and enablement strategies. <sup>[12](#12)</sup>

### Core Components of Managed Adoption

**1. Establish Central AI Governance**

Create a dedicated AI transformation organization or office to govern AI strategy, architecture, tools, trust, and standards across the enterprise. <sup>[12](#12)</sup> This should include:

- **AI Technology Review Board**: A cross-functional governance body to evaluate, approve, and monitor AI platforms, tools, and systems, ensuring alignment with enterprise standards, security protocols, legal, and trust guidelines. <sup>[12](#12)</sup>
- **Clear AI governance policies**: Implement policies and processes to prevent data loss, ensure compliance, and align AI use with organizational objectives. <sup>[12](#12)</sup>
- **AI registry**: Maintain a living inventory of sanctioned models, data connectors, and owners, transforming oversight into asset management. <sup>[13](#13)</sup>

**2. Provide Approved Tools**

Select one or more AI coding assistants that meet your security requirements:

- Enterprise agreements with no-training commitments and strict retention controls
- Clear data handling terms (DPA/SCCs, subprocessors, data residency)
- SOC 2 / ISO 27001 compliance and recent attestations
- Self-hosted or virtual private options for sensitive environments
- Terms of service that meet your legal requirements

**Offer a "choose your own AI" approach within approved boundaries**: Provide a curated menu of vetted GenAI tools tailored to different roles and use cases (e.g., copilots for developers, content tools for marketing, agentic platforms for builders). While employees can choose their preferred tool, IT retains oversight. <sup>[12](#12)</sup>

**3. Implement Technical Controls**

Deploy security measures that work with AI usage:

- **Centralized AI egress proxy** with:
  - Provider/model allowlisting
  - Prompt/response logging
  - PII/secret redaction and context filtering
  - Rate limits and usage analytics
- **IDE-integrated security scanning** and secrets detection for AI suggestions
- **CI/CD gates** that:
  - Flag AI-influenced diffs for enhanced review
  - Enforce SAST/DAST on high-risk changes
  - Run dependency and license scanning (SCA)
- **Code provenance tagging** (e.g., commit trailers or metadata) to mark AI-assisted changes
- **DLP** for clipboard/paste events and outbound traffic
- **Package policy enforcement** and version pinning
- **Automated discovery platforms** to identify and catalog AI systems used within the organization, scan for shadow AI tools, and close visibility gaps. <sup>[12](#12)</sup>

**4. Create Safe Experimentation Environments**

Stand up an **AI labs function**: Provide a sandboxed environment where teams can safely explore new AI platforms and tools, test ideas, and validate use cases before scaling. Labs reduce the need for unsanctioned experimentation while accelerating innovation. <sup>[12](#12)</sup>

AI sandboxes should include:
- Contained environments for testing and validating models
- Synthetic or anonymized data
- Freedom within defined boundaries
- Fast-track process to promote successful experiments to production <sup>[13](#13)</sup>

**5. Train and Set Expectations**

**Promote employee education and awareness**: Train teams on AI risk, trusted AI principles, security best practices, and data handling. <sup>[12](#12)</sup> Include scenario-based examples that clarify gray areas and make responsible use intuitive.

Educate developers on:
- Which tools are approved and how to access them
- What data can and cannot be shared with AI
- How to prompt AI for secure code
- How to review AI-generated code
- When AI is appropriate vs. prohibited (e.g., security-critical code)

**Clearly communicate generative AI policies to employees**: Once internal policies are established, communicate widely what generative AI tools they are free to use, for what purposes, and with what kinds of data. Be as specific as possible about what is safe use and what is not. <sup>[12](#12)</sup>

Publish an AI Acceptable Use Policy and a clear exception/opt-out process.

**6. Foster a Culture of Transparency and Innovation**

**Promote a culture of transparency and innovation**: Make it safe for employees to share how they're using GenAI. Reward responsible innovation and engage teams early in piloting new tools or workflows. <sup>[12](#12)</sup>

Consider an **AI amnesty program**: Declare a period (e.g., 30 days) where employees can register their shadow AI usage without consequences. They share what tools they're using, what data they're processing, what business problems they're solving, and what productivity gains they're seeing. In return, help them transition to sanctioned alternatives or fast-track approval for critical tools. <sup>[11](#11)</sup>

Establish a **Citizen AI Developer Program**: Create citizen AI developers who can evaluate and recommend new AI tools, train colleagues on approved AI usage, act as liaisons between departments and IT, and help shape practical, usable policies. <sup>[11](#11)</sup>

### Program Metrics to Track

Monitor the effectiveness of your managed adoption program:

- % of developers using approved tools
- % of code diffs with AI-influence tags
- Vulnerability density in AI vs. non-AI diffs
- % prompts redacted/blocked by the proxy
- Incidents attributable to AI-generated code; mean time to remediate
- License policy violations caught in CI
- AI tool discovery through inventory systems
- Employee satisfaction with approved AI tools <sup>[12](#12)</sup>

### 30/60/90-Day Managed Adoption Plan

**30 days:**
- Publish AI Acceptable Use Policy (v1)
- Select interim approved tools and tiers (enterprise/API with no-training)
- Stand up an AI proxy with redaction, logging, and allowlisting
- Pilot with 2–3 teams; enable IDE scanning, secrets detection, and SCA in CI
- Launch amnesty program for shadow AI disclosure <sup>[11](#11)</sup>

**60 days:**
- Expand to 30–50% of developers; add CI gates for AI-flagged diffs
- Roll out reviewer training and an AI code review checklist
- Execute DPAs/SCCs; verify provider SOC 2/ISO attestations
- Define metrics and start weekly reporting
- Establish AI labs for safe experimentation <sup>[12](#12)</sup>

**90 days:**
- Org-wide rollout; default to approved tools
- Enforce dependency and package policies
- Quarterly risk reviews; publish program KPIs and learnings
- Deploy automated AI inventory and monitoring systems <sup>[12](#12)</sup>

### Reviewer Checklist for AI-Generated Diffs

When reviewing code that may be AI-generated:

- Inputs/outputs validated and sanitized; prompt injection risks considered
- Authn/z boundaries preserved; least privilege enforced
- Secrets handling correct; no keys or tokens in code or logs
- Crypto: no homegrown algorithms; approved libraries only
- Error handling avoids information leakage
- Dependencies vetted; versions pinned; licenses compatible
- Tests present and meaningful; negative/security tests included

## The Hidden Opportunities in Shadow AI

While shadow AI contains inherent risks, organizations should recognize and harness its potential benefits: <sup>[12](#12)</sup>

- **Accelerated experimentation**: Employees can rapidly test new ideas and workflows, surfacing high-impact use cases organically.
- **Increased engagement and empowerment**: When employees feel trusted to explore new tools, it fuels motivation, creativity, and retention.
- **Real-time insight into system gaps**: The tools people choose reveal friction points in existing AI systems—highlighting where modernization is needed.
- **Tool discovery for future investment**: Shadow AI surfaces emerging tools that can inform enterprise-scale adoption and innovation strategy.
- **Organic upskilling**: Hands-on use of various GenAI tools builds prompting, automation, and critical thinking skills.
- **Grassroots innovation**: Employees often discover novel use cases that top-down strategies might overlook.
- **Cross-functional collaboration**: Shadow AI often emerges in problem-solving teams that span business and technical roles.
- **Pressure-testing enterprise AI strategy**: Shadow AI reveals where official AI systems, tools, policies, or platforms are falling short.

By recognizing and channeling these benefits into sanctioned programs, organizations can turn shadow AI from a liability into a strategic advantage. <sup>[12](#12)</sup>

## The Cost of Doing Nothing

If you ignore the Shadow AI problem, here's what you're accepting:

- **Unknown data exposure** — You don't know what proprietary information has been shared with AI providers <sup>[11](#11)</sup>
- **Unmanaged security risks** — AI-generated vulnerabilities slip into production without oversight <sup>[13](#13)</sup>
- **Legal and compliance exposure** — Potential violations of privacy laws (GDPR fines can reach €20,000,000 or 4% of worldwide revenue), licensing requirements, or industry regulations <sup>[11](#11)</sup>
- **Reputational damage** — Incidents like Sports Illustrated's AI-generated authors or Uber Eats' AI-generated food images undermine consumer trust <sup>[11](#11)</sup>
- **Competitive disadvantage** — While you're fighting adoption, competitors are using AI productively with proper guardrails <sup>[12](#12)</sup>
- **Talent retention issues** — Top developers want to use modern tools; banning them makes your organization less attractive <sup>[11](#11)</sup>
- **Erosion of trust** — Both internally among employees and externally with customers and partners <sup>[12](#12)</sup>

## What's Next?

We've established three key points:

1. **AI coding tools offer real productivity benefits** (Chapter 1)
2. **They also introduce serious security risks** (Chapter 2)
3. **Banning them doesn't work and often makes things worse** (this chapter)

The solution is clear: managed adoption with proper policies, technical controls, and training. Organizations that respond with rigid control will stifle innovation. Those that respond with clear strategy, empowered oversight, and curated choice will unlock AI's full potential—safely and at scale. <sup>[12](#12)</sup>

In the next chapter, we'll explore **The Productivity Paradox** — understanding when AI actually helps versus when it hurts, so you can set realistic expectations and make smart decisions about where to deploy these tools.

## Quick Takeaways

Before moving on, make sure you understand:

- Banning AI tools drives usage underground (Shadow AI) <sup>[13](#13)</sup>
- 44% of employees have used AI in ways that violate policies; 38% have shared sensitive work data without permission <sup>[11](#11) [15](#15)</sup>
- Shadow AI eliminates visibility, control, and security oversight <sup>[12](#12) [13](#13)</sup>
- Organizations lose the ability to monitor data exposure and code quality
- Historical bans on productive tools (email, USB, cloud storage) all failed
- The right approach is managed adoption with approved tools, governance, controls, and training <sup>[12](#12)</sup>
- Turn shadow AI from a liability into an innovation engine through proper enablement <sup>[12](#12)</sup>
- Doing nothing creates unknown risks, compliance violations, and competitive disadvantages

<Woz 
title="Reflection" 
description="Apply this to your context" 
context="Ask user about their organization's current approach to AI tools and help them identify if they might have a Shadow AI problem." 
prompt=""
placeholder="Write to Woz about your organization's current approach to AI tools and help them identify if they might have a Shadow AI problem."
/>

---

## Sources and Further Reading

<a id="1">[1]</a> **GitLab (2024)** – [Global DevSecOps Report: AI in Software Development](https://about.gitlab.com/developer-survey/)

<a id="2">[2]</a> **Pearce et al. (2021)** – [Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions](https://arxiv.org/abs/2108.09293)

<a id="3">[3]</a> **Stanford HAI (2023)** – [Do Users Write More Insecure Code with AI Assistants?](https://hai.stanford.edu/news/do-users-write-more-insecure-code-ai-assistants)

<a id="4">[4]</a> **OpenAI** – [API Data Usage Policies](https://openai.com/policies/api-data-usage-policies)

<a id="5">[5]</a> **OpenAI** – [ChatGPT Enterprise Privacy and Data Control](https://openai.com/enterprise)

<a id="6">[6]</a> **GitHub** – [Copilot for Business: Data Privacy](https://docs.github.com/en/copilot/about-github-copilot-for-business#data-privacy)

<a id="7">[7]</a> **U.S. HHS** – [Guidance Regarding Methods for De-identification of Protected Health Information](https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html)

<a id="8">[8]</a> **CIO.com (2025)** – [Shadow AI: The Hidden Agents Beyond Traditional Governance](https://www.cio.com/article/4083473/shadow-ai-the-hidden-agents-beyond-traditional-governance.html)

<a id="9">[9]</a> **Dell Technologies / Forbes (2025)** – [Powering Possibilities in Healthcare with AI and Edge Computing](https://www.forbes.com/sites/delltechnologies/2025/07/23/powering-possibilities-in-healthcare-with-ai-and-edge-computing/)

<a id="10">[10]</a> **CIO.com (2025)** – [Shadow AI: The Hidden Agents Beyond Traditional Governance - Audit Perspective](https://www.cio.com/article/4083473/shadow-ai-the-hidden-agents-beyond-traditional-governance.html)

<a id="11">[11]</a> **Knight, Michael (2025)** – [Shadow AI: The Problem What Teams Don't Know Can Hurt Them](https://www.linkedin.com/pulse/shadow-ai-problem-what-teams-dont-know-hurt-them-michael-knight-cw76c/)

<a id="12">[12]</a> **KPMG (August 2025)** – [Shadow AI is Already Here: Take Control, Reduce Risk, and Unleash Innovation](https://kpmg.com/us/en/articles/2025/shadow-ai-already-here.html)